{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some smiles\n",
    "\n",
    "The inputs here are simple smiles, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it smiles from moses dataset, which we'll get it to predict character-level (i.e. token in smiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "import pandas as pd\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, content):\n",
    "        chars = sorted(list(set(content)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d smiles, %d unique characters.' % (data_size, vocab_size))\n",
    "    \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.data[idx]\n",
    "        len_smiles = len(smiles)\n",
    "        dix =  [self.stoi[s] for s in smiles]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can download this moses file here https://media.githubusercontent.com/media/molecularsets/moses/master/data/dataset_v1.csv\n",
    "\n",
    "smiles = pd.read_csv('moses.csv')['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some preprocessin, adding \"<\" to make every smile of max length (for us '<' is an end token)\n",
    "lens = [len(i) for i in smiles]\n",
    "max_len = max(lens)\n",
    "smiles = [ i + str('<')*(max_len - len(i)) for i in smiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ' '.join(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1936962 smiles, 28 unique characters.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CharDataset(smiles, content, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/23/2020 20:10:48 - INFO - mingpt.model -   number of parameters: 6.347520e+06\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=256)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 260: train loss 0.59974. lr 5.999643e-04: 100%|██████████| 261/261 [00:33<00:00,  7.69it/s]\n",
      "epoch 2 iter 260: train loss 0.50448. lr 5.998572e-04: 100%|██████████| 261/261 [00:36<00:00,  7.07it/s]\n",
      "epoch 3 iter 260: train loss 0.47783. lr 5.996786e-04: 100%|██████████| 261/261 [00:33<00:00,  7.87it/s]\n",
      "epoch 4 iter 260: train loss 0.43664. lr 5.994287e-04: 100%|██████████| 261/261 [00:35<00:00,  7.39it/s]\n",
      "epoch 5 iter 260: train loss 0.39477. lr 5.991075e-04: 100%|██████████| 261/261 [00:33<00:00,  7.85it/s]\n",
      "epoch 6 iter 260: train loss 0.36882. lr 5.987150e-04: 100%|██████████| 261/261 [00:34<00:00,  7.54it/s]\n",
      "epoch 7 iter 260: train loss 0.35371. lr 5.982514e-04: 100%|██████████| 261/261 [00:35<00:00,  7.43it/s]\n",
      "epoch 8 iter 260: train loss 0.33723. lr 5.977168e-04: 100%|██████████| 261/261 [00:34<00:00,  7.64it/s]\n",
      "epoch 9 iter 260: train loss 0.33420. lr 5.971112e-04: 100%|██████████| 261/261 [00:34<00:00,  7.65it/s]\n",
      "epoch 10 iter 260: train loss 0.31674. lr 5.964349e-04: 100%|██████████| 261/261 [00:32<00:00,  7.93it/s]\n",
      "epoch 11 iter 260: train loss 0.31362. lr 5.956880e-04: 100%|██████████| 261/261 [00:33<00:00,  7.87it/s]\n",
      "epoch 12 iter 260: train loss 0.29854. lr 5.948707e-04: 100%|██████████| 261/261 [00:33<00:00,  7.78it/s]\n",
      "epoch 13 iter 260: train loss 0.29830. lr 5.939832e-04: 100%|██████████| 261/261 [00:34<00:00,  7.52it/s]\n",
      "epoch 14 iter 260: train loss 0.29894. lr 5.930257e-04: 100%|██████████| 261/261 [00:34<00:00,  7.55it/s]\n",
      "epoch 15 iter 60: train loss 0.37425. lr 5.927917e-04:  23%|██▎       | 61/261 [00:07<00:25,  7.83it/s]"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "import math\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=50, batch_size=128, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=32*20, final_tokens=200*len(train_dataset)*block_size,\n",
    "                      num_workers=10)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alright, let's sample some molecules and draw them using rdkit\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.core.display import HTML\n",
    "from rdkit.Chem.QED import qed\n",
    "from rdkit.Chem import PandasTools\n",
    "from mingpt.utils import sample\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df):\n",
    "    return HTML(df.to_html(notebook=True))\n",
    "PandasTools.RenderImagesInAllDataFrames(images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecules = []\n",
    "context = \"C\"\n",
    "for i in range(100):\n",
    "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "    y = sample(model, x, block_size, temperature=0.9, sample=True, top_k=5)[0]\n",
    "    completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "    completion = completion.replace('<', '')\n",
    "    mol = Chem.MolFromSmiles(completion)\n",
    "    if mol:\n",
    "        molecules.append(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Valid molecules % = {}\".format(len(molecules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_dict = []\n",
    "for i in molecules:\n",
    "    mol_dict.append({'molecule' : i, 'qed': qed(i), 'smiles': Chem.MolToSmiles(i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(mol_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(results['qed'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.DataStructs import TanimotoSimilarity\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_list = []\n",
    "for molecule in molecules:\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(molecule, 2, nBits=1024)\n",
    "    fp_list.append(fp)\n",
    "\n",
    "diversity = []\n",
    "for i in range(len(fp_list)):\n",
    "    for j in range(i+1, len(fp_list)):\n",
    "        current_diverity  = 1 - float(TanimotoSimilarity(fp_list[i], fp_list[j]))\n",
    "        diversity.append(current_diverity)\n",
    "\n",
    "\"Diversity of molecules % = {}\".format(np.mean(diversity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
